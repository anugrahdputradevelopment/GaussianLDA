{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import random\n",
    "import pprint\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from gensim.models import Word2Vec \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "wordvecs = Word2Vec.load_word2vec_format(\n",
    "    \"/Users/michael/Documents/Gaussian_LDA-master/data/glove.wiki/glove.6B.50d.txt\", binary=False)\n",
    "data = datasets.fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), subset=\"train\",\n",
    "                                   categories=['comp.windows.x', 'sci.med',\n",
    "                                               'talk.politics.mideast', 'soc.religion.christian',\n",
    "                                               'sci.space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data[0]['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(wordvecs.vocab.keys())\n",
    "stopwords = set(nltk.corpus.stopwords.words(fileids='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138\n"
     ]
    }
   ],
   "source": [
    "most = 0\n",
    "for doc in txt:\n",
    "    if len(doc) > 2000: most += 1\n",
    "print most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plaindoc = \"would one like to eat a banana but\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'to', 'eat', 'a', 'banana', 'but']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x != \"would\" and x != \"one\", plaindoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would', 'one', 'like', 'eat', 'banana']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(lambda x: x not in stopwords, plaindoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would', 'one', 'like', 'to', 'eat', 'a', 'banana', 'but']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plaindoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandocs = []\n",
    "for doc in txt:\n",
    "    doc = doc.split()\n",
    "#     doc = [word for word in doc if word in vocab]\n",
    "#     doc = [word for word in doc if not word in stopwords]\n",
    "#     doc = [word for word in doc if word.isalpha()]\n",
    "    doc = filter(lambda x: x in vocab, doc)\n",
    "    doc = filter(lambda x: x not in stopwords, doc)\n",
    "    doc = filter(lambda x: x.isalpha(), doc)\n",
    "    doc = filter(lambda x: x != \"would\" and x != \"one\" and x !='like' and x !='even', doc)\n",
    "    doc = ' '.join(doc)\n",
    "    cleandocs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2943"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleandocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleandocs = [doc for doc in cleandocs if len(doc) < 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 20 ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for doc in cleandocs:\n",
    "    for word in doc.split():\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word =='even':\n",
    "        print 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = nltk.probability.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'people', 690),\n",
       " (u'know', 610),\n",
       " (u'get', 608),\n",
       " (u'think', 566),\n",
       " (u'also', 494),\n",
       " (u'could', 458),\n",
       " (u'use', 430),\n",
       " (u'may', 426),\n",
       " (u'see', 386),\n",
       " (u'much', 365)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = itemgetter(*np.random.random_integers(0, 1650, 600).tolist())\n",
    "a  = bb(cleandocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.insert(0, test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"hockey basketball baseball football sports touchdown golf playoffs \" * 1000\n",
    "a.append(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc.startswith(\"hockey basketball baseball football sports touchdown golf playoffs \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = itemgetter(*np.random.random_integers(0, 1650, 500).tolist())\n",
    "len(bb(cleandocs))\n",
    "\n",
    "f = '/Users/michael/Documents/GaussianLDA/clean20news.txt'\n",
    "# np.savetxt(f, docs)\n",
    "with open(f, 'w') as fi:\n",
    "    for doc in a:\n",
    "        try:\n",
    "            fi.write(\"%s\\n\" % doc)\n",
    "        except UnicodeEncodeError: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing to make sure we can read it well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = '/Users/michael/Documents/GaussianLDA/clean20news.txt'\n",
    "# with open(f, 'r') as fi:\n",
    "#     text = fi.read().splitlines()\n",
    "#     fi.close()\n",
    "# print len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b, c = [0,0], [0,0], [0,0]\n",
    "\n",
    "for x, y in zip(txt, data['target']): \n",
    "    if y == 0: a[0] += len(x); a[1] += 1\n",
    "    if y == 1: b[0] += len(x); b[1] += 1\n",
    "    if y == 2: c[0] += len(x); c[1] += 1\n",
    "        \n",
    "print a[0] / a[1], a[1]\n",
    "print b[0] / b[1], b[1]\n",
    "print c[0] / c[1], c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mansour_topic = np.loadtxt(\"/Users/michael/Documents/GaussianLDA/output/1iter4topic2Topic Mean.txt\")\n",
    "# das_topic = np.loadtxt(\"/Users/michael/Documents/Gaussian_LDA-master/output/Aliaswiki50D20T3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecs = []\n",
    "vocab = set([])\n",
    "for doc in cleandocs:\n",
    "    for word in doc.split():\n",
    "        vocab.add(word)\n",
    "        \n",
    "for word in vocab:\n",
    "    try: vecs.append(wordvecs[word])\n",
    "    except: None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=15, n_init=10,\n",
       "    n_jobs=-1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=15, n_jobs=-1)\n",
    "km.fit(np.array(vecs))\n",
    "wordvecs.most_similar(positive=[km.cluster_centers_[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making More Sythetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n",
      "truck cars vehicle driver driving bus vehicles parked motorcycle taxi passenger pickup trucks cab suv train drivers bicycle jeep airplane sciences research institute studies physics psychology scientific biology studying chemistry study economics mathematics journalism university engineering professor humanities sociology philosophy cash paying funds pay raise paid billions millions get fund keep tax savings credit make putting taxes spend making giving\n"
     ]
    }
   ],
   "source": [
    "%pprint on\n",
    "topic1 = ' '.join([word for word, score in wordvecs.most_similar(positive=['car'], topn=20)])\n",
    "topic2 = ' '.join([word for word, score in wordvecs.most_similar(positive=['science'], topn=20)])\n",
    "topic3 = ' '.join([word for word, score in wordvecs.most_similar(positive=['money'], topn=20)])\n",
    "print topic1, topic2, topic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bus': 0.8210511207580566,\n",
       " u'cars': 0.8870189189910889,\n",
       " u'driver': 0.8464018702507019,\n",
       " u'driving': 0.8384189009666443,\n",
       " u'motorcycle': 0.7866503000259399,\n",
       " u'parked': 0.7902189493179321,\n",
       " u'taxi': 0.7833929061889648,\n",
       " u'truck': 0.92085862159729,\n",
       " u'vehicle': 0.8833683729171753,\n",
       " u'vehicles': 0.8174992799758911}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word:score for word, score in wordvecs.most_similar(positive=['car'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = [topic1, topic2, topic3]\n",
    "\n",
    "f = '/Users/michael/Documents/GaussianLDA/clean20news.txt'\n",
    "# np.savetxt(f, docs)\n",
    "with open(f, 'w') as fi:\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            fi.write(\"%s\\n\" % doc)\n",
    "        except UnicodeEncodeError: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current project:\n",
    "\n",
    "you have unlabeled cans of paint.  You want to be able to assign color names to them.\n",
    "\n",
    "My model currently just dumps all the cans of paint into a bucket, mixes it up, and labels each paint can that gross dark brown purlply color that comes out when a child mixes all the paints together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing kmeans cluster centroids\n",
    "\n",
    "topic1 = [word for word, score in wordvecs.most_similar(positive=['car'], topn=20)]\n",
    "topic2 = [word for word, score in wordvecs.most_similar(positive=['science'], topn=20)]\n",
    "topic3 = [word for word, score in wordvecs.most_similar(positive=['money'], topn=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrs = []\n",
    "for doc in [topic1, topic2, topic3]:\n",
    "    for word in doc:\n",
    "        arrs.append(wordvecs[word])\n",
    "arrs = np.array(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "km = KMeans(n_clusters=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'money', 0.95746248960495), (u'pay', 0.9299221038818359), (u'raise', 0.9211190342903137), (u'paying', 0.9172735810279846), (u'cash', 0.9055809378623962), (u'cost', 0.8760908246040344), (u'funds', 0.8710236549377441), (u'giving', 0.8627578020095825), (u'paid', 0.861911416053772), (u'costs', 0.8605511784553528)]\n",
      "[(u'science', 0.9424635171890259), (u'studies', 0.920809268951416), (u'psychology', 0.9107574224472046), (u'sciences', 0.9034210443496704), (u'biology', 0.8869606256484985), (u'mathematics', 0.8854325413703918), (u'physics', 0.8730563521385193), (u'institute', 0.8722023963928223), (u'studying', 0.8703454732894897), (u'anthropology', 0.8632563352584839)]\n",
      "[(u'car', 0.9455156326293945), (u'truck', 0.9394636750221252), (u'vehicle', 0.9090796709060669), (u'cars', 0.9007307291030884), (u'trucks', 0.8714910745620728), (u'vehicles', 0.8700135350227356), (u'bus', 0.8697482943534851), (u'taxi', 0.8601557016372681), (u'passenger', 0.851845383644104), (u'cab', 0.8493589162826538)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print wordvecs.most_similar(positive=[km.cluster_centers_[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "Start looking at the pdf's of each word and make sure they're being assigned to the right topics... even though the actual topics might be wrong, lets make sure that it is fairly consistant. \n",
    "\n",
    "set a break point for certain intervals, and use the debugger console to check the values. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
